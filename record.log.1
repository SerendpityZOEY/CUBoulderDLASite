((1, u'fadfsa', u'', 1, u'fdaf@yahoo.com', u'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', u'fadsf', u'null', u'null', u'null', u'null'), (2, u'dfsag', u'gfs', 1, u'fad', u'bfds', u'daf', u'gfds', u'fgds', u'fsg', u'g'), (3, u'Creating Tactile Graphics for Blind People', u'Computer Science', 42, None, u'The Superhuman Computing Lab is creating new technology to help blind and visually impaired individuals see images, such as photographs, charts, and illustrations. Students on this project will develop web-based or Android applications to translate visual information to haptic feedback using the TPad Tablet. Students will also test the efficacy of these techniques through user studies with blind and sighted users. Students should have prior programming experience in Java or HTML/Javascript. ', u'', u'', u'', u'', u''), (4, u'Auditory pattern learning and recognition for autonomous robotic perception', u'Aerospace Engineering Sciences', 43, None, u'Autonomous mobile robots are constrained by size, weight and power to carry limited sensing payloads, most often active/passive vision sensors (i.e. lidar or cameras) or active sonar. As a result, robots are often \u201cdeaf\u201d to ambient sounds, which can be a very valuable source information in indoor or urban environments. This research project will develop hardware and software for processing, learning and recognizing ambient sounds aboard an autonomous mobile robot, so that it can identify, localize and capture moving targets in a simulated indoor environment while playing a game of \u201cCops and Robbers\u201d. The first portion of this project will consist of hardware development, sensor design and signal processing to acquire sounds from the ambient environment. These signals will then be used in tandem with machine learning algorithms to recognize and characterize a variety of useful sound features encountered during the \u201cCops and Robbers\u201d game. This will allow the robot to update its uncertain understanding of the world as it plays the game with both passive vision and aural sensors. \r\n\r\n', u'Familiarity with basic acoustic signal processing, hardware design, and basic ROS/Python programming are required', u'', u'', u'', u''))
[['fadfsa', 'ffff', '4', '', 'fdaf@yahoo.com', 'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', 'fadsf']]
[['fadfsa', 'ffff', '4', '', 'fdaf@yahoo.com', 'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', 'fadsf'], ['dfsag', 'ffff', '4', 'gfs', 'fad', 'bfds', 'daf']]
[['fadfsa', 'ffff', '4', '', 'fdaf@yahoo.com', 'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', 'fadsf'], ['dfsag', 'ffff', '4', 'gfs', 'fad', 'bfds', 'daf'], ['Creating Tactile Graphics for Blind People', 'Shaun Kane', 'Computer Science', 'Computer Science', 'None', 'The Superhuman Computing Lab is creating new technology to help blind and visually impaired individuals see images, such as photographs, charts, and illustrations. Students on this project will develop web-based or Android applications to translate visual information to haptic feedback using the TPad Tablet. Students will also test the efficacy of these techniques through user studies with blind and sighted users. Students should have prior programming experience in Java or HTML/Javascript. ', '']]
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in teacher page
waiting for input in teacher page
waiting for input in teacher page
waiting for input in teacher page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in student page
waiting for input in teacher page
waiting for input in teacher page
((1, u'fadfsa', u'', 1, u'fdaf@yahoo.com', u'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', u'fadsf', u'null', u'null', u'null', u'null'), (2, u'dfsag', u'gfs', 1, u'fad', u'bfds', u'daf', u'gfds', u'fgds', u'fsg', u'g'), (3, u'Creating Tactile Graphics for Blind People', u'Computer Science', 42, None, u'The Superhuman Computing Lab is creating new technology to help blind and visually impaired individuals see images, such as photographs, charts, and illustrations. Students on this project will develop web-based or Android applications to translate visual information to haptic feedback using the TPad Tablet. Students will also test the efficacy of these techniques through user studies with blind and sighted users. Students should have prior programming experience in Java or HTML/Javascript. ', u'', u'', u'', u'', u''), (4, u'Auditory pattern learning and recognition for autonomous robotic perception', u'Aerospace Engineering Sciences', 43, None, u'Autonomous mobile robots are constrained by size, weight and power to carry limited sensing payloads, most often active/passive vision sensors (i.e. lidar or cameras) or active sonar. As a result, robots are often \u201cdeaf\u201d to ambient sounds, which can be a very valuable source information in indoor or urban environments. This research project will develop hardware and software for processing, learning and recognizing ambient sounds aboard an autonomous mobile robot, so that it can identify, localize and capture moving targets in a simulated indoor environment while playing a game of \u201cCops and Robbers\u201d. The first portion of this project will consist of hardware development, sensor design and signal processing to acquire sounds from the ambient environment. These signals will then be used in tandem with machine learning algorithms to recognize and characterize a variety of useful sound features encountered during the \u201cCops and Robbers\u201d game. This will allow the robot to update its uncertain understanding of the world as it plays the game with both passive vision and aural sensors. \r\n\r\n', u'Familiarity with basic acoustic signal processing, hardware design, and basic ROS/Python programming are required', u'', u'', u'', u''))
[['fadfsa', 'ffff', '4', '', 'fdaf@yahoo.com', 'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', 'fadsf']]
[['fadfsa', 'ffff', '4', '', 'fdaf@yahoo.com', 'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', 'fadsf'], ['dfsag', 'ffff', '4', 'gfs', 'fad', 'bfds', 'daf']]
[['fadfsa', 'ffff', '4', '', 'fdaf@yahoo.com', 'fadsa tgtgt ewfwferg reg rg reg er gr r gg t gt gt gtrtg r gre e fe wf  w few few f ewf ew fwe f fe ew few f ef ewf  c re r ferf e fer fer f ef', 'fadsf'], ['dfsag', 'ffff', '4', 'gfs', 'fad', 'bfds', 'daf'], ['Creating Tactile Graphics for Blind People', 'Shaun Kane', '2', 'Computer Science', 'None', 'The Superhuman Computing Lab is creating new technology to help blind and visually impaired individuals see images, such as photographs, charts, and illustrations. Students on this project will develop web-based or Android applications to translate visual information to haptic feedback using the TPad Tablet. Students will also test the efficacy of these techniques through user studies with blind and sighted users. Students should have prior programming experience in Java or HTML/Javascript. ', '']]
